<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>ECALELF: Instructions for ALCARAW and ALCARECO production</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="doxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<!-- Generated by Doxygen 1.6.1 -->
<div class="navigation" id="top">
  <div class="tabs">
    <ul>
      <li><a href="index.html"><span>Main&nbsp;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&nbsp;Pages</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li><a href="dirs.html"><span>Directories</span></a></li>
    </ul>
  </div>
</div>
<div class="contents">


<h1><a class="anchor" id="page2">Instructions for ALCARAW and ALCARECO production </a></h1><h2><a class="anchor" id="ALCARECO_production">
ALCARECO production</a></h2>
<p>Steps for the production using the CERN batch system (launch from lxplus):</p>
<p>0) Check that there are no updates in the repository, this will merge automatically the updates in the master branch </p>
<div class="fragment"><pre class="fragment"> git pull 
</pre></div><p>1) Fill the alcareco_datasets.dat with the information of the desired run range It is convenient to add the new line at the bottom of the file. Please refere to the <a class="el" href="DATAFORMATS.html">Dataformats</a> file for information about how the alcareco_datasets.dat file is structured.</p>
<p>For this step, in case of update of a growing dataset, please have a look to the "Production update" section below</p>
<p>2) </p>
<div class="fragment"><pre class="fragment">   ./scripts/prodAlcareco.sh `parseDatasetFile.sh alcareco_datasets.dat |grep 22Jan | grep Run2012A` --tutorial
</pre></div> <div class="fragment"><pre class="fragment">   ./scripts/prodAlcareco.sh `parseDatasetFile.sh alcareco_datasets.dat |grep RelVal22Jan | grep Run2012A` --tutorial
</pre></div><p> use the --tutorial if you are testing the machinery: it will save the files in a separate directory on EOS (not the central one) use grep commands to select the proper line from alcareco_datasets.dat the script will create the crab task and submit it use --createOnly or --submitOnly if needed use --check to launch the check of the job results use the --isMC is the dataset is MC use the -s skim: ZSkim, WSkim, EleSkim for the preselection (ZSkim usually) use --scheduler=remoteGlidein if the dataset is not at T2_CH_CERN and you want to use the grid</p>
<p>see ./scripts/prodAlcareco.sh --help for more informations</p>
<p>3) ./scripts/prodAlcaraw.sh `parseDatasetFile.sh alcaraw_datasets.dat |grep 22Jan | grep Run2012A` --tutorial --check check the job status and output</p>
<p>4) Commit the changes to alcaraw_datasets.dat! </p>
<div class="fragment"><pre class="fragment">   git add alcareco_datasets.dat
   git commit -m <span class="stringliteral">&quot;message&quot;</span> 
</pre></div><div class="fragment"><pre class="fragment">

OUTDATED instructions
#============================== ALCARECO and ALCARAW production instructions


#### ALCARAW private production  -&gt; TO BE UPDATED
The following instructions correspond to point: 1a), 1.1a), 2a) indicated in 
Calibration/README:
https://svnweb.cern.ch/cern/wsvn/analysis/trunk/Calibration/README

For the detailed content of alcaraw format consult the Calibration/doc/alcaraw.dump file

1) Fill the alcaraw_datasets.dat with the information of the desired run range
   It is convenient to add the new line at the bottom of the file.
   For this step, in case of update of a growing dataset, please have
   a look to the "Production update" section below

2) ./scripts/prodAlcaraw.sh `parseDatasetFile.sh alcaraw_datasets.dat |grep 22Jan | grep Run2012A` --tutorial
   use the --tutorial if you are testing the machinery
   use grep commands to select the proper line from alcaraw_datasets.dat 
   the script will create the crab task and submit it
   use --createOnly or --submitOnly if needed
   see   ./scripts/prodAlcaraw.sh --help  for more informations
   
3) ./scripts/prodAlcaraw.sh `parseDatasetFile.sh alcaraw_datasets.dat |grep 22Jan | grep Run2012A` --tutorial --check
   check the job status and output

4) Commit the changes to alcaraw_datasets.dat!
   svn ci -m "message" 














######### OBSOLETE INSTRUCTIONS
4a) Produce the ALCARECO: (if you have put the new line as last line)
   ./scripts/prodAlcareco.sh `parseProdSandboxOptions.sh alcaraw_datasets.dat | tail -1` --submit --scheduler lsf
   (otherwise)
   parseProdSandboxOptions.sh alcaraw_datasets.dat
   copy the output for the line you are interested in
   ./scripts/prodAlcareco.sh [PAST THE LINE] --submit --scheduler lsf

	If you are producing ALCARECO for a line added to
        alcareco_datasets.dat, just replace alcaraw_datasets.dat with
        alcareco_datasets.dat
	FOR MC: you need to add the --isMC option to the command
4b) Produce the ALCARAW: (if you have put the new line as last line)
   ./scripts/prodSandbox.sh `parseProdSandboxOptions.sh alcaraw_datasets.dat | tail -1` --submit --scheduler lsf
   (otherwise)
   parseProdSandboxOptions.sh alcaraw_datasets.dat
   copy the output for the line you are interested in
   ./scripts/prodSandbox.sh [PAST THE LINE] --submit --scheduler lsf

   To produce launching jobs on the GRID, change --scheduler lsf in --scheduler glite or --scheduler glidein

4c) To check the staus of the jobs submitted do:
   for dir in prod_alcareco/DoubleElectron-RUN2012A-May10ReReco-v1/*/; do echo $dir; ./scripts/resubmitCrab.sh -u $dir; done

   The script resubmitCrab.sh checks the exit code of each single job and it resubmits the jobs if the exit code is different from 0 except for in some cases (for particular    exit code)



5) The final output will be put:
   in the storage element indicated in the alcaraw_datasets.dat or alcareco_datasets.dat file
   in the subdir of the store/ directory indicated in the alcaraw_datasets.dat or alcareco_datasets.dat
   and following the structure indicated below:
   (STORAGE_ELEMENT)/(REMOTE_DIR)/ENERGY/(DATASETNAME)/(RUNRANGE)
	where ENERGY is 7TeV for 2011 datasets and 8TeV for 2012 datasets
        where the elements indicated in parethesys are taken from the alcaraw_datasets.dat file (or alcareco_datasets.dat)
        the REMOTE_DIR ends with sandbox/ if you are producing ALCARAW and 
        alcareco/ for ALCARECO 
    by default the output dir is the T2_CH_CERN: 
    /eos/cms/store/group/alca_ecalcalib/[alcareco/sandbox]/[7TeV/8TeV]/DATASETNAME/RUNRANGE

*** EXPERT: where REMOTE_DIR is specified by the --remote_dir option, ENERGY is 8TeV or 7TeV depending on the datasetpath, NAME is specified by the -n option and RUN_RANGE by the -r option

7) If the jobs have finished and everything is ok, commit the file
   alcaraw_datasets.dat where the line added is not commented to inform
   that the production has finished  

#============================== UPDATE ALCARECO and ALCARAW for growing dataset
In order to keep updated the ALCARECO and ALCARAW for Double and
Single electron samples follow this instructions. The production will
be in-sync with the availability of the new golden JSON file.

1) check that the new JSON is available executing the script 
   ./scripts/updateLastDataset.sh
   The output gives the following informations:

-lastRun is the last run processed
-max(run.run_number) is the last run in database
-lastWeekRun is the last run not modified since four days at least
-the last JSON

It has to be verified that the lastWeekRun is greater than the last run in JSON.
If it is not verified it is better to wait before proceeding. 
N.B: max(run.run_number) has to be always greater than the last run in JSON

Then, copy and past the last updated line from the
alcaraw_datasets.dat and change the run range accordingly

</pre></div><div class="fragment"><pre class="fragment">
Testing the alcaraw + alcareco production
cmsRun ALCARAW_RECO/python/alcaSkimming.py files=file://../../../B292A7F8-3A7F-E111-A1A5-001D09F29114.root maxEvents=6 &amp;&gt; log2.log

Testing the alcarereco production
cmsRun ALCARAW_RECO/python/alcaSkimming.py files=file://alcaraw.root maxEvents=4 type=ALCARERECO tagFile=ALCARAW_RECO/config/reRecoTags/Cal_Jun2013_AlphaComb_v1.py &amp;&gt; log.log

#============================== RERECO
Instructions:

To run the Rereco with a new set of IC tags it is necessary to have a configuration file corresponding to those tags.

An example of a configuration file:

##############################################################
import FWCore.ParameterSet.Config as cms
from CondCore.DBCommon.CondDBSetup_cfi import *

#### Please fill with comments
# Basic tag combination for 2012 end year seminar conditions
# Laser, alpha tags are fixed: no time to improve them
# A set of IC are derived on top of them and will be tested:
# Cal_Nov2012_ICEle_v2:
# description
#

RerecoGlobalTag = cms.ESSource("PoolDBESSource",
                               CondDBSetup,
                               connect = cms.string('frontier://FrontierProd/CMS_COND_31X_GLOBALTAG'),
                               globaltag = cms.string('GR_R_53_V13::All'),
                               toGet = cms.VPSet(
    cms.PSet(record = cms.string("EcalLaserAPDPNRatiosRcd"),
             tag = cms.string('EcalLaserAPDPNRatios_data_20120814_2011-2012_v3_upd20120919'),
             connect = cms.untracked.string("frontier://FrontierProd/CMS_COND_42X_ECAL_LAS")
             )
    ,cms.PSet(record = cms.string('EcalLaserAlphasRcd'),
              tag = cms.string('EcalLaserAlphas_EB_sic1_btcp152_EE_sic1_btcp116'),
              connect = cms.untracked.string('frontier://FrontierInt/CMS_COND_ECAL')
              )
    ,cms.PSet(record = cms.string("EcalIntercalibConstantsRcd"),
              tag = cms.string('EcalIntercalibConstants_V20120922_Ele19092012_2012IncEtaScale8TeV'),
              connect = cms.untracked.string("frontier://FrontierInt/CMS_COND_ECAL")
              )
    ),
                               BlobStreamerName = cms.untracked.string('TBufferBlobStreamingService')
                               )

#############################################################


Where, in this case, the IC tag is EcalIntercalibConstants_V20120922_Ele19092012_2012IncEtaScale8TeV.
When the rereco has to be run with a new IC tag, a new file has to be created and it will be similar to the one above except for the section:

cms.PSet(record = cms.string("EcalIntercalibConstantsRcd"),
              tag = cms.string('EcalIntercalibConstants_V20120922_Ele19092012_2012IncEtaScale8TeV'),
              connect = cms.untracked.string("frontier://FrontierInt/CMS_COND_ECAL")
              )

It has to be modified with the replacement of the IC tag inside tag = cms.string('...')
These files are in the directory:
ALCARAW_RECOconfig/reRecoTags
The convention is to name these configuration files in this way: tag_name.py

So, for the rereco production follow the steps below:

1. create the tag_name.py in ALCARAW_RECOconfig/reRecoTags directory

2. commit the file in svn:
svn add config/reRecoTags/tag_name.py
svn ci -m "...." config/reRecoTags/tag_name.py

3. Define the period in alcaraw_datasets.dat if not already defined

4. run the script RerecoQuick.sh giving two parameters: period and tag_name
./scripts/RerecoQuick.sh  -p period -t config/reRecoTags/tag_name.py --scheduler=lsf --rereco
For example: ./scripts/RerecoQuick.sh -p Cal_Nov2012 -t config/reRecoTags/Cal_Nov2012_ICcombAllR9_v2.py --rereco --scheduler=lsf
 
More info:
For each dataset defined by the period,
the script 
 - makes the list of ALCARAW files present on EOS
 - creates a crab cfg file: tmp/sandboxRereco.cfg
 - execute the command crab -cfg tmp/sandboxRereco.cfg -create
 - cracks the list of input files for crab putting the ALCARAW files list
 - execute the command crab -c sandboxRereco/_taskDir_/ -submit
 - waits for 50 minutes
 - check if the jobs are exited with status 0, if not resubmit the failed jobs
 - re-check every 10 minutes
 - stops only if finishes all jobs with status 0

5. check the exit status of the jobs:
./script/checkAndResubmitRereco.sh -t config/reRecoTags/tag_name.py
  This command will loop over all folders corresponding to the different
  datasets and check if jobs are exited with status 0 


6. if all fine, commit the alcarereco_datasets.dat file
svn status -u
svn update
svn ci -m "rereco tag_name done"






</pre></div> </div>
<hr size="1"/><address style="text-align: right;"><small>Generated on 24 Jun 2014 for ECALELF by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.6.1 </small></address>
</body>
</html>
